{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use black formatter\n",
    "%load_ext lab_black\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# Print with 3 decimals\n",
    "np.set_printoptions(formatter={\"float\": lambda x: \"{0:0.3f}\".format(x)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejecicio #1:    Normalización\n",
    "Muchos algoritmos de Machine Learning necesitan datos de entrada centrados y normalizados. Una normalización habitual es el z-score, que implica restarle la media y dividir por el desvío a cada feature de mi dataset. \n",
    "\n",
    "Dado un dataset X de n muestras y m features, implementar un método en numpy para normalizar con z-score. Pueden utilizar np.mean() y np.std()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[45.891, 48.999, 1.189, -32.194, -64.689],\n",
       "       [-16.647, -12.208, -23.461, -55.561, -18.228],\n",
       "       [-95.130, -15.046, -11.519, -22.661, -27.956],\n",
       "       [-65.405, -74.439, -78.136, 54.615, -17.066],\n",
       "       [-27.964, -34.725, 52.074, -26.783, 12.328]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_data = np.random.uniform(low=-100, high=100, size=(5, 5))\n",
    "random_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.627, 1.663, 0.314, -0.420, -1.678],\n",
       "       [0.318, 0.132, -0.274, -1.046, 0.198],\n",
       "       [-1.324, 0.061, 0.011, -0.165, -0.195],\n",
       "       [-0.702, -1.424, -1.579, 1.905, 0.244],\n",
       "       [0.081, -0.431, 1.528, -0.275, 1.431]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize(data):\n",
    "    \"\"\"Normalize data using z-score formula\"\"\"\n",
    "    return (random_data - data.mean(axis=0)) / data.std(axis=0)\n",
    "\n",
    "\n",
    "normalize(random_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejecicio #2:    Remover filas y columnas con NaNs en un dataset\n",
    "Dado un dataset, hacer una función que, utilizando numpy, filtre las columnas y las filas que tienen NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.670, 2.864, nan, 2.949, nan, -9.365, 7.565],\n",
       "       [13.505, 4.482, nan, 0.771, nan, -3.706, 32.867],\n",
       "       [-5.737, -1.031, nan, 0.908, nan, 5.333, -20.922],\n",
       "       [-0.019, 1.910, nan, 0.137, nan, 3.400, 1.433],\n",
       "       [6.080, 1.528, nan, 0.746, nan, -11.487, 11.868]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"/tf/notebooks/CEIA-inteligencia_artificial/clase_3/clase3v2.csv\"\n",
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = list(csv.reader(f, delimiter=\";\"))\n",
    "\n",
    "data = np.array(data, dtype=float)\n",
    "data[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño original: (100, 7)\n",
      "Tamaño sin NaN en columnas: (100, 5)\n",
      "Tamaño sin NaN en filas: (75, 7)\n"
     ]
    }
   ],
   "source": [
    "remove_nan_column = lambda data: data[:, ~np.isnan(data).any(axis=0)]\n",
    "remove_nan_row = lambda data: data[~np.isnan(data).any(axis=1)]\n",
    "\n",
    "print(f\"Tamaño original: {data.shape}\")\n",
    "print(f\"Tamaño sin NaN en columnas: {remove_nan_column(data).shape}\")\n",
    "print(f\"Tamaño sin NaN en filas: {remove_nan_row(data).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejecicio #3:    Reemplazar NaNs por la media de la columna\n",
    "Dado un dataset, hacer una función que utilizando numpy reemplace los NaNs por la media de la columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.670, 2.864, 4.185, 2.949, -0.631, -9.365, 7.565],\n",
       "       [13.505, 4.482, 4.185, 0.771, -0.631, -3.706, 32.867],\n",
       "       [-5.737, -1.031, 4.185, 0.908, -0.631, 5.333, -20.922],\n",
       "       [-0.019, 1.910, 4.185, 0.137, -0.631, 3.400, 1.433],\n",
       "       [6.080, 1.528, 4.185, 0.746, -0.631, -11.487, 11.868]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def replace_nan_with_mean(data):\n",
    "    \"\"\"Replace NaN values with mean of the column\"\"\"\n",
    "    mean = np.nanmean(data, axis=0)\n",
    "    return np.nan_to_num(data, nan=mean)\n",
    "\n",
    "\n",
    "replace_nan_with_mean(data)[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejecicio #4:    Dado un dataset X separarlo en 70 / 20 / 10\n",
    "Como vimos en el ejercicio integrador, en problemas de Machine Learning es fundamental que separemos los datasets de n muestras, en 3 datasets de la siguiente manera:\n",
    "\n",
    "* Training dataset: los datos que utilizaremos para entrenar nuestros modelos. Ej: 70% de las muestras.\n",
    "* Validation dataset: los datos que usamos para calcular métricas y ajustar los hiperparámetros de nuestros modelos. Ej: 20% de las muestras.\n",
    "* Testing dataset: una vez que entrenamos los modelos y encontramos los hiperparámetros óptimos de los mísmos, el testing dataset se lo utiliza para computar las métricas finales de nuestros modelos y analizar cómo se comporta respecto a la generalización. Ej: 10% de las muestras.\n",
    "\n",
    "A partir de utilizar np.random.permutation, hacer un método que dado un dataset, devuelva los 3 datasets como nuevos numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataset train: (70, 7)\n",
      "Tamaño del dataset validation: (20, 7)\n",
      "Tamaño del dataset test: (10, 7)\n"
     ]
    }
   ],
   "source": [
    "def split_data(data, train_size=0.8, validation_size=None):\n",
    "    \"\"\"Split data into train, validation and test sets. Validation in optional\"\"\"\n",
    "\n",
    "    if validation_size is None:\n",
    "        if train_size > 1:\n",
    "            raise ValueError(\"Train size must be less than 1\")\n",
    "    else:\n",
    "        if train_size + validation_size > 1:\n",
    "            raise ValueError(\"Train size + validation size must be less than 1\")\n",
    "\n",
    "    suffled_data = np.random.permutation(data)\n",
    "    data_samples = data.shape[0]\n",
    "    train_number = int(data_samples * train_size)\n",
    "\n",
    "    if validation_size:\n",
    "        validation_number = train_number + int(data_samples * validation_size)\n",
    "        return (\n",
    "            suffled_data[0:train_number],\n",
    "            suffled_data[train_number:validation_number],\n",
    "            suffled_data[validation_number:],\n",
    "        )\n",
    "    else:\n",
    "        return suffled_data[0:train_number], suffled_data[train_number:]\n",
    "\n",
    "\n",
    "train, validation, test = split_data(data, train_size=0.7, validation_size=0.2)\n",
    "print(f\"Tamaño del dataset train: {train.shape}\")\n",
    "print(f\"Tamaño del dataset validation: {validation.shape}\")\n",
    "print(f\"Tamaño del dataset test: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, train_size=0.8, test_size=0.2, validation=False):\n",
    "    \"\"\"Split data into train, validation and test sets. Validation in optional\"\"\"\n",
    "\n",
    "    if train_size + test_size > 1 or train_size < 0 or test_size < 0:\n",
    "        raise ValueError(\n",
    "            \"train_size + test_size must be less than 1 and greater than 0\"\n",
    "        )\n",
    "\n",
    "    suffled_data = np.random.permutation(data)\n",
    "    data_samples = data.shape[0]\n",
    "    train_number = int(data_samples * train_size)\n",
    "    test_number = 0\n",
    "    validation_number = 0\n",
    "\n",
    "    if validation:\n",
    "        test_number = int(data_samples * test_size)\n",
    "        validation_number = data_samples - train_number - test_number\n",
    "        return suffled_data[0:train_number], suffled_data[train_number:validation_number], suffled_data[validation_number:]\n",
    "    else:\n",
    "        return suffled_data[0:train_number], suffled_data[train_number:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio #5:   A partir del dataset de consigna, aplicar los conceptos de regresión lineal.\n",
    "1. Armar una clase para cargar el [dataset](data/income.csv) en un ndarray estructurado, tal como se realizó en el ejercicio 10 de la Clase 1.\n",
    "2. Incluir un método split a la clase para obtener los sets de training y test.\n",
    "3. Crear una clase métrica base y una clase MSE (Error cuadrático medio) que herede de la clase base.\n",
    "4. Crear una clase modelo base y clases regresión lineal y regresión afín que hereden de la primera. Usar los conocimientos teóricos vistos en clase.\n",
    "5. Hacer un fit de las regresiones con los datos de entrenamiento.\n",
    "6. Hacer un predict sobre los datos de test y reportar el MSE en cada caso.\n",
    "7. Graficar la curva obtenida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

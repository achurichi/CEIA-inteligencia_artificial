{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use black formatter\n",
    "# %load_ext lab_black\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Print with 3 decimals\n",
    "np.set_printoptions(formatter={\"float\": lambda x: \"{0:0.3f}\".format(x)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejecicio #1:    Normalización\n",
    "Muchos algoritmos de Machine Learning necesitan datos de entrada centrados y normalizados. Una normalización habitual es el z-score, que implica restarle la media y dividir por el desvío a cada feature de mi dataset. \n",
    "\n",
    "Dado un dataset X de n muestras y m features, implementar un método en numpy para normalizar con z-score. Pueden utilizar np.mean() y np.std()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.779, 31.454, 89.124, -58.735, -38.642],\n",
       "       [-68.140, 21.035, 5.128, 73.546, -91.042],\n",
       "       [-45.432, -94.573, 98.578, -90.645, -62.258],\n",
       "       [44.073, 12.400, -4.349, 56.431, 31.074],\n",
       "       [-73.521, 90.542, 58.101, -57.411, 53.585]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_data = np.random.uniform(low=-100, high=100, size=(5, 5))\n",
    "random_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.716, 0.321, 0.942, -0.648, -0.312],\n",
       "       [-0.894, 0.148, -1.046, 1.329, -1.262],\n",
       "       [-0.393, -1.780, 1.166, -1.125, -0.740],\n",
       "       [1.584, 0.004, -1.270, 1.073, 0.953],\n",
       "       [-1.013, 1.307, 0.208, -0.629, 1.361]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize(data):\n",
    "    \"\"\"Normalize data using z-score formula\"\"\"\n",
    "    return (data - data.mean(axis=0)) / data.std(axis=0)\n",
    "\n",
    "\n",
    "normalize(random_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejecicio #2:    Remover filas y columnas con NaNs en un dataset\n",
    "Dado un dataset, hacer una función que, utilizando numpy, filtre las columnas y las filas que tienen NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.670, 2.864, nan, 2.949, nan, -9.365, 7.565],\n",
       "       [13.505, 4.482, nan, 0.771, nan, -3.706, 32.867],\n",
       "       [-5.737, -1.031, nan, 0.908, nan, 5.333, -20.922],\n",
       "       [-0.019, 1.910, nan, 0.137, nan, 3.400, 1.433],\n",
       "       [6.080, 1.528, nan, 0.746, nan, -11.487, 11.868]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"/tf/notebooks/CEIA-inteligencia_artificial/clase_3/clase3v2.csv\"\n",
    "\n",
    "\n",
    "def load_dataset_from_file(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = list(csv.reader(f, delimiter=\";\"))\n",
    "    return np.array(data, dtype=float)\n",
    "\n",
    "\n",
    "data = load_dataset_from_file(file_path)\n",
    "data[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño original: (100, 7)\n",
      "Tamaño sin NaN en columnas: (100, 5)\n",
      "Tamaño sin NaN en filas: (75, 7)\n"
     ]
    }
   ],
   "source": [
    "remove_nan_column = lambda data: data[:, ~np.isnan(data).any(axis=0)]\n",
    "remove_nan_row = lambda data: data[~np.isnan(data).any(axis=1)]\n",
    "\n",
    "print(f\"Tamaño original: {data.shape}\")\n",
    "print(f\"Tamaño sin NaN en columnas: {remove_nan_column(data).shape}\")\n",
    "print(f\"Tamaño sin NaN en filas: {remove_nan_row(data).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejecicio #3:    Reemplazar NaNs por la media de la columna\n",
    "Dado un dataset, hacer una función que utilizando numpy reemplace los NaNs por la media de la columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.670, 2.864, 4.185, 2.949, -0.631, -9.365, 7.565],\n",
       "       [13.505, 4.482, 4.185, 0.771, -0.631, -3.706, 32.867],\n",
       "       [-5.737, -1.031, 4.185, 0.908, -0.631, 5.333, -20.922],\n",
       "       [-0.019, 1.910, 4.185, 0.137, -0.631, 3.400, 1.433],\n",
       "       [6.080, 1.528, 4.185, 0.746, -0.631, -11.487, 11.868]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def replace_nan_with_mean(data):\n",
    "    \"\"\"Replace NaN values with mean of the column\"\"\"\n",
    "    mean = np.nanmean(data, axis=0)\n",
    "    return np.nan_to_num(data, nan=mean)\n",
    "\n",
    "\n",
    "replace_nan_with_mean(data)[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejecicio #4:    Dado un dataset X separarlo en 70 / 20 / 10\n",
    "Como vimos en el ejercicio integrador, en problemas de Machine Learning es fundamental que separemos los datasets de n muestras, en 3 datasets de la siguiente manera:\n",
    "\n",
    "* Training dataset: los datos que utilizaremos para entrenar nuestros modelos. Ej: 70% de las muestras.\n",
    "* Validation dataset: los datos que usamos para calcular métricas y ajustar los hiperparámetros de nuestros modelos. Ej: 20% de las muestras.\n",
    "* Testing dataset: una vez que entrenamos los modelos y encontramos los hiperparámetros óptimos de los mísmos, el testing dataset se lo utiliza para computar las métricas finales de nuestros modelos y analizar cómo se comporta respecto a la generalización. Ej: 10% de las muestras.\n",
    "\n",
    "A partir de utilizar np.random.permutation, hacer un método que dado un dataset, devuelva los 3 datasets como nuevos numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del dataset train: (70, 7)\n",
      "Tamaño del dataset validation: (20, 7)\n",
      "Tamaño del dataset test: (10, 7)\n"
     ]
    }
   ],
   "source": [
    "def split_data(data, train_size=0.8, validation_size=None):\n",
    "    \"\"\"Split data into train, validation and test sets. Validation in optional\"\"\"\n",
    "\n",
    "    if validation_size is None:\n",
    "        if train_size > 1:\n",
    "            raise ValueError(\"Train size must be less than 1\")\n",
    "    else:\n",
    "        if train_size + validation_size > 1:\n",
    "            raise ValueError(\"Train size + validation size must be less than 1\")\n",
    "\n",
    "    suffled_data = np.random.permutation(data)\n",
    "    data_samples = data.shape[0]\n",
    "    train_number = int(data_samples * train_size)\n",
    "\n",
    "    if validation_size:\n",
    "        validation_number = train_number + int(data_samples * validation_size)\n",
    "        return (\n",
    "            suffled_data[0:train_number],\n",
    "            suffled_data[train_number:validation_number],\n",
    "            suffled_data[validation_number:],\n",
    "        )\n",
    "    else:\n",
    "        return suffled_data[0:train_number], suffled_data[train_number:]\n",
    "\n",
    "\n",
    "train, validation, test = split_data(data, train_size=0.7, validation_size=0.2)\n",
    "print(f\"Tamaño del dataset train: {train.shape}\")\n",
    "print(f\"Tamaño del dataset validation: {validation.shape}\")\n",
    "print(f\"Tamaño del dataset test: {test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio #5:   A partir del dataset de consigna, aplicar los conceptos de regresión lineal.\n",
    "1. Cargar los datos con objeto de clase Data (implementada por ustedes) con un método que cumpla esa función al pasarle la ruta. Hacer un split de los datos en train/test (usar 80/20)\n",
    "Tratar los nans con al menos dos de las técnicas vistas en clase. (pasarían a tener dos datasets para comparar en lo que sigue)\n",
    "2. Utilizar PCA para quedarse con las 3 CP.  (de cada uno del punto 1, idealmente usen su implementación, pero pueden usar las librerías)\n",
    "3. Crear una clase métrica base y una clase MSE que herede es ella. (esto viene de ejercicios anteriores)\n",
    "4. Crear una clase modelo base y clase regresión lineal que herede de ella.  (esto viene de ejercicios anteirores)\n",
    "5. Entrenar la regresión lineal sobre train. Calcular MSE sobre validation. (para todas las variantes que hayan hecho en 2) y comparar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    \"\"\"\n",
    "    Class to load and prepare data for machine learning algorithms\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file):\n",
    "        self.load_data(file)\n",
    "\n",
    "    def load_data(self, file):\n",
    "        self.original_data = load_dataset_from_file(file)\n",
    "        self.data = self.original_data\n",
    "        return self.data\n",
    "\n",
    "    def fill_nans(self):\n",
    "        self.data = replace_nan_with_mean(self.data)\n",
    "\n",
    "    def remove_nans(self):\n",
    "        self.data = remove_nan_column(self.data)\n",
    "\n",
    "    def normalize(self):\n",
    "        self.data = normalize(self.data)\n",
    "\n",
    "    def prepare_data(self, nan_strategy=\"fill\"):\n",
    "        if nan_strategy == \"fill\":\n",
    "            self.fill_nans()\n",
    "        elif nan_strategy == \"remove\":\n",
    "            self.remove_nans()\n",
    "        self.normalize()\n",
    "\n",
    "    def split_data(self, train_size=0.8, validation_size=None):\n",
    "        return split_data(\n",
    "            self.data, train_size=train_size, validation_size=validation_size\n",
    "        )\n",
    "\n",
    "    def restore_data(self):\n",
    "        self.data = self.original_data\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.data\n",
    "\n",
    "\n",
    "data = Data(file_path)\n",
    "strategies = [\"fill\", \"remove\"]\n",
    "dataArray = []\n",
    "\n",
    "for strategy in strategies:\n",
    "    data.prepare_data(nan_strategy=strategy)\n",
    "    train, test = data.split_data(train_size=0.8)\n",
    "    dataArray.append(\n",
    "        {\n",
    "            \"train\": {\n",
    "                \"X\": train[:, :-1],\n",
    "                \"y\": train[:, -1],\n",
    "            },\n",
    "            \"test\": {\n",
    "                \"X\": test[:, :-1],\n",
    "                \"y\": test[:, -1],\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "    data.restore_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrega \"X_pca\" de 3 componentes a los datos de train y test\n",
    "for data in dataArray:\n",
    "    for key in data:\n",
    "        X_pca = PCA(n_components=3).fit_transform(data[key][\"X\"])\n",
    "        data[key][\"X_pca\"] = X_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseMetric:\n",
    "    \"\"\"\n",
    "    Abstract class for metrics\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self):\n",
    "        raise NotImplementedError(\"__call__ method not implemented\")\n",
    "\n",
    "\n",
    "class MSE(BaseMetric):\n",
    "    \"\"\"\n",
    "    Abstract class for metrics\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\"Mean Squared Error\")\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        return np.sum(y_true - y_pred) ** 2 / y_true.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"\n",
    "    Abstract class for models\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "\n",
    "\n",
    "class LinearRegression(Model):\n",
    "    \"\"\"\n",
    "    Class for Linear Regression Model\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        try:\n",
    "            self.model = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "        except:\n",
    "            self.model = np.linalg.pinv(X.T @ X) @ X.T @ y\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X @ self.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 1.97504 (NaN promediados)\n",
      "MSE: 0.29347 (NaN eliminados)\n"
     ]
    }
   ],
   "source": [
    "linearRegression = LinearRegression()\n",
    "mse = MSE()\n",
    "\n",
    "for data in dataArray:\n",
    "    linearRegression.fit(data[\"train\"][\"X_pca\"], data[\"train\"][\"y\"])\n",
    "    y_pred = linearRegression.predict(data[\"test\"][\"X_pca\"])\n",
    "    data[\"test\"][\"MSE\"] = mse(data[\"test\"][\"y\"], y_pred)\n",
    "\n",
    "print(f'MSE: {dataArray[0][\"test\"][\"MSE\"]:.5f} (NaN promediados)')\n",
    "print(f'MSE: {dataArray[1][\"test\"][\"MSE\"]:.5f} (NaN eliminados)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizando el experimento repetidas veces, se puedo observar que los valores de MSE varian mucho dependiendo de como se haya hecho el split. Dando en algunos casos, mejores resultados con el primer dataset y en en otros casos, mejores resultados con el segundo.\n",
    "\n",
    "A continuación, se pueden obsrvar los resultados obtenidos para distintas ejecuciones de código, donde lo único que varía es el split de los datos (siempre utilizando proporciones 80/20).\n",
    "\n",
    "| N Ejecución | MSE (NaN promediados) | MSE (NaN eliminados) |\n",
    "| :----: | :----: | :----: |\n",
    "| 1 | 0.50681 | 0.20909 |\n",
    "| 2 | 0.44160 | 1.81363 |\n",
    "| 3 | 0.45217 | 1.02649 |\n",
    "| 4 | 0.15246 | 2.61569 |\n",
    "| 5 | 1.97504 | 0.29347 |\n",
    "\n",
    "Probablemente esto se deba a que la muestra es muy pequeña y no es suficientemente representativa de la población de la cual probiene.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

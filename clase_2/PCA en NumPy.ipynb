{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación de PCA en NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos\n",
    "* Implementación de PCA en NumPy paso a paso\n",
    "* Comparación de resultados con Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use black formatter\n",
    "# %load_ext lab_black\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA as SklearnPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dado un dataset $X \\in \\mathbb{R}^{n, d}$, con $n$ muestras y $d$ features, queremos reducir sus dimensiones a $m$. Para ello, el primer paso es centrar el dataset (Hint: usen np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.12719469,  0.08470232, -0.44863779,  0.00920519, -0.40952666],\n",
       "       [-0.07620155,  0.27900397, -0.02912941, -0.24172075, -0.08545116],\n",
       "       [-0.18128711, -0.53777687,  0.13518053, -0.12975136, -0.38734422],\n",
       "       [ 0.38223733,  0.04800193,  0.27454986, -0.15366669,  0.18206238],\n",
       "       [-0.06659283, -0.14569582,  0.04592173, -0.27459537,  0.45049785],\n",
       "       [-0.28021031,  0.30820375,  0.18497272,  0.03703955,  0.40990637],\n",
       "       [ 0.10580334,  0.27887353, -0.33716434, -0.02364514, -0.11007374],\n",
       "       [-0.33973419, -0.01287795,  0.24588706, -0.19469135, -0.49407078],\n",
       "       [ 0.20017935, -0.29258074, -0.240896  ,  0.55194595,  0.13695499],\n",
       "       [ 0.38300067, -0.00985412,  0.16931563,  0.41987996,  0.30704498]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10\n",
    "d = 5\n",
    "m = 3\n",
    "X = np.random.uniform(size=(n, d))\n",
    "norm_X = X - np.mean(X, axis=0)\n",
    "norm_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Obtener la matriz de covarianza de $X^T$, revisar en la teoría por qué utilizamos la transpuesta. Buscar en la documentación de NumPy qué funciones se pueden utilizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06636477, -0.0023684 , -0.00197248,  0.03609834,  0.03941926],\n",
       "       [-0.0023684 ,  0.07293106, -0.00930077, -0.0136167 ,  0.01686489],\n",
       "       [-0.00197248, -0.00930077,  0.06588205, -0.01826059,  0.02388602],\n",
       "       [ 0.03609834, -0.0136167 , -0.01826059,  0.07723827,  0.02599478],\n",
       "       [ 0.03941926,  0.01686489,  0.02388602,  0.02599478,  0.12204703]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov_mat = np.cov(norm_X.T)\n",
    "cov_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Calcular los autovalores y autovectores de la matriz de covarianza. Revisar la documentación de NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.16318074, 0.0342833 , 0.02720429, 0.10046836, 0.07932649]),\n",
       " array([[-0.46365911,  0.80317257, -0.25834784,  0.27049314,  0.00489022],\n",
       "        [-0.06315572,  0.2160573 ,  0.38902658, -0.3927399 ,  0.80233677],\n",
       "        [-0.10146272,  0.28960477,  0.55001334, -0.49774253, -0.59629866],\n",
       "        [-0.39962133, -0.234209  ,  0.63563339,  0.61706613,  0.02546611],\n",
       "        [-0.78169028, -0.41171384, -0.2745364 , -0.37956633, -0.00334438]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eig_values, eig_vectors = np.linalg.eig(cov_mat)\n",
    "eig_values, eig_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Ordernar los autovectores en el sentido de los autovalores decrecientes, revisar la teoría de ser necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.46365911,  0.27049314,  0.00489022,  0.80317257, -0.25834784],\n",
       "       [-0.06315572, -0.3927399 ,  0.80233677,  0.2160573 ,  0.38902658],\n",
       "       [-0.10146272, -0.49774253, -0.59629866,  0.28960477,  0.55001334],\n",
       "       [-0.39962133,  0.61706613,  0.02546611, -0.234209  ,  0.63563339],\n",
       "       [-0.78169028, -0.37956633, -0.00334438, -0.41171384, -0.2745364 ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_eig_values = np.argsort(eig_values)[::-1]\n",
    "sorted_eig_vectors = eig_vectors[:, sorted_eig_values]\n",
    "sorted_eig_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Proyectar el dataset centrado sobre los $m$ autovectores más relevantes (Hint: usen np.dot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.41558997,  0.31675758,  0.33646392],\n",
       "       [ 0.18405951, -0.23241234,  0.23498242],\n",
       "       [ 0.45893794,  0.16184208, -0.5149815 ],\n",
       "       [-0.2890239 , -0.2160421 , -0.12785296],\n",
       "       [-0.20699707, -0.32408686, -0.15310534],\n",
       "       [-0.24353222, -0.42163847,  0.1351863 ],\n",
       "       [ 0.06303315,  0.11410502,  0.42508451],\n",
       "       [ 0.59739894, -0.14183133, -0.16192163],\n",
       "       [-0.37752062,  0.5775631 , -0.07652552],\n",
       "       [-0.60194569,  0.16574333, -0.09733021]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_eig_vectors = sorted_eig_vectors[:, :m]\n",
    "proy_ds = np.dot(norm_X, m_eig_vectors)\n",
    "proy_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Consolidar los pasos anteriores en una función o clase PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.20410476, -0.26977774],\n",
       "       [ 0.09866819,  0.24047168],\n",
       "       [-0.11402813,  0.42601736],\n",
       "       [-0.81694402, -0.20032535],\n",
       "       [ 0.18196192,  0.17046892],\n",
       "       [ 0.06203172,  0.19147816],\n",
       "       [-0.06620393, -0.24165323],\n",
       "       [ 0.45040949, -0.3166798 ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PCA:\n",
    "    \"\"\"\n",
    "    Principal Component Analysis\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, X, m):\n",
    "        \"\"\"\n",
    "        Reduce the dimensionality of the data X from (n)x(d) to (n)x(m) using PCA\n",
    "        \"\"\"\n",
    "        norm_X = X - np.mean(X, axis=0)\n",
    "        cov_mat = np.cov(norm_X.T)\n",
    "        eig_values, eig_vectors = np.linalg.eig(cov_mat)\n",
    "        sorted_eig_values = np.argsort(eig_values)[::-1]\n",
    "        sorted_eig_vectors = eig_vectors[:, sorted_eig_values]\n",
    "        return np.dot(norm_X, sorted_eig_vectors[:, :m])\n",
    "\n",
    "\n",
    "X = np.random.uniform(size=(8, 4))\n",
    "\n",
    "pca = PCA()\n",
    "pca(X, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Comparar los resultados obtenidos con el modelo de PCA implementado en Scikit-learn ([ver documentación](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)). Tomar como dataset:\n",
    "\n",
    "$X=\\begin{bmatrix}\n",
    "0.8 & 0.7\\\\\n",
    "0.1 & -0.1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Se debe reducir a un componente. Verificar los resultados con np.testing.assert_allclose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My PCA\n",
      " [[-0.53150729]\n",
      " [ 0.53150729]]\n",
      "\n",
      "Sklearn PCA\n",
      " [[-0.53150729]\n",
      " [ 0.53150729]]\n",
      "\n",
      "\u001b[32mTest passed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0.8, 0.7], [0.1, -0.1]])\n",
    "n, d = X.shape\n",
    "m = 1\n",
    "\n",
    "pca = PCA()\n",
    "pca_result = pca(X, m)\n",
    "\n",
    "sklearn_pca = SklearnPCA(n_components=m)\n",
    "sklearn_pca_result = sklearn_pca.fit_transform(X)\n",
    "\n",
    "pca_result, sklearn_pca_result\n",
    "\n",
    "print(\"My PCA\\n\", pca_result)\n",
    "print()\n",
    "print(\"Sklearn PCA\\n\", sklearn_pca_result)\n",
    "print()\n",
    "\n",
    "np.testing.assert_allclose(pca_result, sklearn_pca_result, atol=1e-5)\n",
    "print(\"\\x1b[32mTest passed\\x1b[0m\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
